## Learning rates:      [2e-5, 5e-6, 2e-6]
## Batch size:          [32/16, 16/8, 4/4]
## Bert base:           110 M
## Bert large:          340 M         <-- large
## openai gpt:          110 M
## GPT2:                117 M         <-- weird large
## XLM:                 >= 295 M      <-- super large
## XLnet:               110 M
## XLNet large:         340 M         <-- super large
## roberta:             125 M
## roberta large:       355 M         <-- large
## distilbert:          60 M          <-- small

alphanli:
  albert:
    albert-xxlarge-v2:
      lr: 5e-6
      batch_size: 16
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 16
  distilbert:
    distilbert-base-uncased:
      do_lower_case: true
      lr: 5e-5
      batch_size: 128
      max_nb_epochs: 6
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 16
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 16
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.6
    batch_size: 48
    max_seq_len: 128
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
    do_lower_case: false
hellaswag:
  albert:
    albert-xxlarge-v2:
      lr: 5e-6
      batch_size: 8
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 8
  gpt2:
    gpt2:
      lr: 5e-6
      batch_size: 8
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 8
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 8
  distilbert:
    distilbert-base-uncased:
      do_lower_case: true
      lr: 5e-5
      batch_size: 64
      max_nb_epochs: 6
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.5
    batch_size: 16
    max_seq_len: 160
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
    do_lower_case: false
physicaliqa:
  albert:
    albert-xxlarge-v2:
      lr: 5e-6
      batch_size: 8
  libert:
    /auto/nlg-05/chengham/libert/output:
      lr: 2e-5
      batch_size: 32
    bert-base-uncased:
      lr: 2e-5
      batch_size: 8
      accumulate_grad_batches: 4
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 8
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 8
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 8
  distilbert:
    distilbert-base-uncased:
      do_lower_case: true
      lr: 5e-5
      batch_size: 128
      max_nb_epochs: 6
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.5
    batch_size: 32
    max_seq_len: 128
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
    do_lower_case: false
socialiqa:
  albert:
    albert-xxlarge-v2:
      lr: 5e-6
      batch_size: 8
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 8
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 8
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 8
  distilbert:
    distilbert-base-uncased:
      do_lower_case: true
      lr: 5e-5
      batch_size: 128
      max_nb_epochs: 6
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.5
    batch_size: 32
    max_seq_len: 128
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
    do_lower_case: false
vcrqa:
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 8
  gpt2:
    gpt2:
      lr: 5e-6
      batch_size: 8
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 8
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 8
  distilbert:
    distilbert-base-uncased:
      do_lower_case: true
      lr: 5e-5
      batch_size: 128
      max_nb_epochs: 6
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.5
    batch_size: 32
    max_seq_len: 128
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
    do_lower_case: false
vcrqr:
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 8
  gpt2:
    gpt2:
      lr: 5e-6
      batch_size: 8
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 8
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 8
  distilbert:
    distilbert-base-uncased:
      do_lower_case: true
      lr: 5e-5
      batch_size: 128
      max_nb_epochs: 6
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.5
    batch_size: 32
    max_seq_len: 128
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
    do_lower_case: false
qqp:
  albert:
    albert-xxlarge-v2:
      lr: 5e-6
      batch_size: 8
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 8
  gpt2:
    gpt2:
      lr: 5e-6
      batch_size: 8
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 8
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 8
  distilbert:
    distilbert-base-uncased:
      do_lower_case: true
      lr: 5e-5
      batch_size: 64
      max_nb_epochs: 6
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.5
    batch_size: 16
    max_seq_len: 160
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
    do_lower_case: false
